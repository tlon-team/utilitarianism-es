---
title: "Utilitarianism and Practical Ethics"
date: 2023-01-29
draft: false
menu: "main"
weight: 6
description: "Utilitarianism has important implications for how we should think about leading an ethical life. Despite giving no intrinsic weight to deontic constraints, it supports many commonsense prohibitions and virtues in practice. Its main practical difference instead lies in its emphasis on positively doing good, in more expansive and efficient ways than people typically prioritize."
gradientTop: "#089FD1"
gradientBottom: "#305D9C"
---

> Are we to extend our concern to all the beings capable of pleasure and
> pain whose feelings are affected by our conduct? or are we to confine
> our view to human happiness? The former view is the one adopted by...
> the Utilitarian school... it seems arbitrary and unreasonable to
> exclude from the end, as so conceived, any pleasure of any sentient
> being. <br> \- [Henry
> Sidgwick](/utilitarian-thinker/henry-sidgwick)[^1]

{{< TOC >}}

## Introduction

Utilitarianism has important implications for how we should think about
leading an ethical life. In this chapter, we focus on five of its
theoretical implications. First, unlike many other ethical theories,
utilitarianism does not regard actions and omissions as morally
different. Second, it is unusually demanding: it requires us to
sacrifice more than many other ethical theories do. Third, it implies
that we should be cause-impartial: that we should apply our altruistic
efforts to wherever we can have the most positive impact on others.
Fourth, it urges us to consider the well-being of individuals regardless
of what country they live in, what species they belong to, and at what
point in time they exist. Fifth, despite differing radically from
commonsense morality as an approach to ethics, utilitarianism generally
does endorse commonsense moral prohibitions.

We discuss how utilitarians should act in practice in the article
[Acting on Utilitarianism](/acting-on-utilitarianism). In brief, most
utilitarians should [donate a significant portion of their
income](/acting-on-utilitarianism#charitable-giving) to address the
world’s most pressing problems, [devote their careers to doing
good](/acting-on-utilitarianism#career-choice), and aspire to high
degrees of cooperativeness, personal integrity, and honesty.

## Is There a Difference Between Doing and Allowing Harm?

> _"A person may cause evil to others not only by his actions but by his
> inaction, and in either case he is justly accountable to them for the
> injury."_ <br> \- [John Stuart
> Mill](/utilitarian-thinker/john-stuart-mill),
> <cite>[@Mill1859Liberty]</cite>

Many non-consequentialists believe there is a morally relevant
difference between _[doing harm and allowing
harm](https://plato.stanford.edu/entries/doing-allowing/)_, even if the
consequences of an action or inaction are the same. This position is
known as the “Doctrine of Doing and Allowing”, according to which harms
caused by actions—by things we actively do—are worse than harms of
omission. Those who subscribe to this doctrine may, for instance, claim
that it is worse to harm a child, all else being equal, than it is to
fail to prevent the same child from being harmed in an accident.

Of course, all else is typically not equal. From the perspective of
consequentialists and non-consequentialists alike, a societal norm
allowing people to harm children would be worse than one allowing people
to neglect preventing children from being harmed accidentally. This is
because actively harming children could set a precedent encouraging
others to harm children more, which would have worse overall
consequences. Doing harm may well be _instrumentally_ worse than
allowing harm even if the _Doctrine of Doing and Allowing_ is false.

However, while consequentialists—including utilitarians—accept that
doing harm is typically instrumentally worse than allowing harm, they
deny that doing harm is intrinsically worse than allowing harm. For
utilitarians, only the outcome experienced by the child is what matters
in itself. To the harmed child, the result is the same—whether you do
the harming, or someone else does and you fail to prevent it. When
considered from the perspective of the child, the action-omission
distinction is irrelevant: whether their suffering results from your
deliberate action or your neglect, they suffer the same either way.

It matters a great deal whether or not there is an intrinsic moral
difference between doing and allowing harm. As pointed out above, the
_Doctrine of Doing and Allowing_ is at the heart of the disagreement
between many consequentialists and non-consequentialists. Furthermore,
it matters for real-world decision-making. For instance, the ethics
guidelines of many medical associations never allow doctors to actively
and intentionally cause the death of a patient. However, it is
acceptable for doctors to intentionally let a patient die under certain
circumstances, such as if the patient is in great pain and wants to end
their life. This distinction—between a doctor letting a terminally ill
patient die and a doctor actively assisting a patient who wants to end
their life—is regarded as less relevant from a utilitarian perspective.
If we allow doctors to let a terminally ill patient (who wishes to end
their suffering) die, a utilitarian would argue that doctors should also
be permitted to actively assist a patient to bring about their death
with their consent.

## Demandingness

Utilitarianism is a very
[demanding](/objections-to-utilitarianism/demandingness) ethical theory:
it maintains that any time you can do more to help other people than you
can to help yourself, you should do so.[^2] For example, if you could
sacrifice your life to save the lives of several other people then,
other things being equal, according to utilitarianism, you ought to do
so.

Though occasions where sacrificing your own life is the best thing to do
are rare, utilitarianism is still very demanding in the world today. For
example, by [donating to a highly effective global health
charity](/acting-on-utilitarianism#charitable-giving), you can save a
child’s life for just a few thousand dollars.[^3] As long as the benefit
others receive from such donations exceeds what you would gain from
keeping the money for yourself—as they almost certainly do, if you are a
typical citizen of an affluent country—you should give it away. Indeed,
you should probably donate the majority of your lifetime income.
According to utilitarianism, it is only truly justifiable to spend money
on yourself—such as by going out to the movies or buying nice clothes—if
you think that this expenditure would do more good than any possible
donation. This is a very high bar.

As well as recommending very significant donations, utilitarianism
claims that you ought to [choose whatever career will most benefit
others](/acting-on-utilitarianism#career-choice), too. This might
involve non-profit work, conducting important research, or going into
politics or advocacy.

## Cause Impartiality

The prevailing view on helping others is that whom we should help and
how we should help them is a matter of personal preference. On this
common view, one may choose whether to focus on education, the arts,
endangered species, or some other cause on the basis of one’s personal
passions.[^4]

However, utilitarians argue that we should not choose our focus based
primarily on personal attachment to a social cause; instead, we should
apply our focus wherever we can have the most positive impact on others.
Utilitarianism entails what we may call _cause impartiality_:[^5]

> **Cause impartiality is the view that one’s choice of social cause to
> focus on should depend on, and only on, the expected amount of good
> that one can do in that cause.**

To illustrate this idea, suppose that you could donate to one of two
different charities. One provides bednets to protect children from
malaria; the other treats cancer patients. Suppose that you will save
twice as many lives by donating to the bednet charity than by donating
to the cancer charity; however, a family member died of cancer, so you
have a personal attachment to that cause. Should you, therefore, give to
the cancer charity?

The utilitarian would argue not. On the utilitarian view, the importance
of saving two lives rather than one outweighs the personal attachment
that might bring the donor to prioritize the life of the cancer
sufferers. From the viewpoint of utilitarianism, we should be completely
impartial in deciding which social cause to support, and instead let
this decision be driven only by the question of where we can do the most
good.

Importantly, we know that some ways of benefiting individuals do much
more good than others. For example, within the cause of global health
and development, some interventions are over 100 times as effective as
others.[^6] Furthermore, many researchers believe that the difference in
expected impact among _causes_ is as great as the differences among
_interventions within a particular cause_. If so, focusing on the very
best causes is vastly more impactful than focusing on average ones.

## The Expanding Moral Circle

A crucial question in deciding which cause to dedicate our efforts to
regards which individuals we should include in our moral deliberations.

We now recognize that characteristics like race, gender, and sexual
orientation do not justify discriminating against individuals or
disregarding their suffering. Over time, our society has gradually
expanded our moral concern to ever more groups, a trend of moral
progress often called the _expanding moral circle_.[^7] But what are the
limits of this trend? Should the moral circle include all humans but
stop there? Should it be expanded to include non-human animals as well?
Or should it ultimately extend even to plants and the natural
environment?

Utilitarianism provides a clear response to this question: We should
extend our moral concern to all _sentient beings_, meaning every
individual capable of experiencing positive or negative conscious
states. This includes humans and probably many non-human animals, but
not plants or other entities that are non-sentient. This view is
sometimes called _sentiocentrism_ as it regards sentience as the
characteristic that entitles individuals to moral concern. Justifying
this perspective, Peter Singer writes:

> The capacity for suffering and enjoying things is a prerequisite for
> having interests at all, a condition that must be satisfied before we
> can speak of interests in any meaningful way. It would be nonsense to
> say that it was not in the interests of a stone to be kicked along the
> road by a child. A stone does not have interests because it cannot
> suffer. Nothing that we can do to it could possibly make any
> difference to its welfare. A mouse, on the other hand, does have an
> interest in not being tormented, because mice will suffer if they are
> treated in this way...
>
> _If a being suffers, there can be no moral justification for refusing
> to take that suffering into consideration._ No matter what the nature
> of the being, the principle of equality requires that the suffering be
> counted equally with the like suffering—in so far as rough comparisons
> can be made—of any other being. If a being is not capable of
> suffering, or of experiencing enjoyment or happiness, there is nothing
> to be taken into account. This is why the limit of sentience... is the
> only defensible boundary of concern for the interests of others.[^8]

On this basis, a priority for utilitarians may be to help society to
continue to widen its moral circle of concern. For instance, we may want
to persuade people that they should help not just those in their own
country, but also those on the other side of the world; not just those
of their own species but all sentient creatures; and not just people
currently alive but any people whose lives they can affect, including
those in generations to come.

### Cosmopolitanism: Expanding the Moral Circle Across Geography

According to utilitarianism, geographical distance and national
membership are not intrinsically morally relevant. This means that, by
the lights of utilitarianism, we have no grounds for discriminating
against someone because of where they live, where they come from, or
what nationality they have. This makes utilitarianism an example of
_[moral
cosmopolitanism](https://plato.stanford.edu/entries/cosmopolitanism/#TaxoContCosm)_.
Proponents of moral cosmopolitanism believe that if you have the means
to save a life in a faraway country, doing so matters just as much as
saving a life close by in your own country; all lives deserve equal
moral consideration, wherever they are.

Of course, the geographical distance between oneself and one’s
beneficiary may matter instrumentally—it is often easier to help people
close by than people far away. However, in an increasingly globalized
world it has become much easier to benefit even those who live on the
other side of the world. And because of extreme global economic
inequalities, an additional unit of resources benefits people in the
least-developed countries much more than people in affluent countries
like the United States or the United Kingdom—potentially 100 to 1,000
times more.[^9]

We discuss the implications of cosmopolitanism for ethical action in the
article [Acting on
Utilitarianism](/acting-on-utilitarianism#global-health-and-development).

### Anti-Speciesism: Expanding the Moral Circle Across Species

Utilitarianism cares not only about the well-being of humans, but also
about the well-being of non-human animals. Consequently, utilitarianism
rejects
_[speciesism](https://www.animal-ethics.org/ethics-animals-section/speciesism/)_:
the practice of giving some individuals less moral consideration than
others or treating them worse based on their species membership. To give
individuals moral consideration is simply to consider how one’s behavior
will affect them, whether by action or omission. As [Peter
Singer](/utilitarian-thinker/peter-singer) describes it:

> Racists violate the principle of equality by giving greater weight to
> the interests of members of their own race when there is a clash
> between their interests and the interests of those of another race.
> Sexists violate the principle of equality by favoring the interests of
> their own sex. Similarly, speciesists allow the interests of their own
> species to override the greater interests of members of other species.
> The pattern is identical in each case.[^10]

There is a growing scientific consensus that many non-human animals are
sentient,[^11] though not necessarily to the same degree. This includes
most vertebrates, such as mammals, birds and fish, and potentially some
invertebrates, such as octopodes or even insects. These animals can feel
pleasure and pain, and these experiences are morally relevant from a
utilitarian perspective.

Rejecting speciesism entails giving _equal moral consideration_ to the
well-being of all individuals, but does not entail treating all species
equally. Species membership is not morally relevant _in itself_, but
individuals belonging to different species may differ in other ways that
do matter morally. In particular, it is likely that individuals from
different species do not have the same capacity for conscious
experience—for instance, because of the differing numbers of neurons in
their brains. Since utilitarians believe that only sentience matters
morally in itself, the utilitarian concern for individuals is
proportional to their capacity for conscious experience. It is perfectly
consistent with a rejection of speciesism to say we should equally
consider the well-being of a fish and a chimpanzee, without implying
that they have the capacity to suffer to the same degree.

From the utilitarian perspective, what matters _intrinsically_ is the
well-being of individual sentient beings, not the survival of the
species, or the integrity of the ecosystem, or of nature. Individuals
can suffer, while a “species”, an “ecosystem”, or “nature” cannot. Of
course, the survival of species and the integrity of ecosystems and of
nature may well be important instrumentally, to the extent that they
contribute to the well-being of individuals.

Speciesism underlies the current exploitation of billions of non-human
animals by humans. Animals are widely seen as resources: they are raised
and slaughtered for food, used for clothing, or exploited for their
work. These practices [often result in the animals experiencing extreme
suffering](https://www.animal-ethics.org/animal-exploitation-introduction/).

However, not all animal suffering is caused by humans. There are many
more wild animals living in nature than there are domesticated
animals.[^12] In contrast to the widespread romanticized view of nature,
wild animals generally live short lives in harsh environments, and they
experience suffering from many sources including predation, disease,
parasites, exposure to extreme heat or cold, hunger, thirst, and
malnutrition. Against this background, it would be wrong to consider
only the well-being of domesticated animals which humans actively harm,
while ignoring the well-being of wild animals which humans merely allow
to be harmed.[^13] As noted earlier, for the utilitarian, the
distinction between doing and allowing harm is irrelevant. Therefore,
from the utilitarian viewpoint, we should care equally about the welfare
of domestic and wild animals. That said, we currently know little about
how to systematically improve the lives of wild animals. By contrast,
reducing society’s consumption of factory-farmed meat, or improving
conditions on factory farms, would yield clear and enormous benefits for
animals.[^14]

We discuss the implications of rejecting speciesism for ethical action
in the article [Acting on
Utilitarianism](/acting-on-utilitarianism#farm-animal-welfare).

### Longtermism: Expanding the Moral Circle Across Time

From the utilitarian perspective, people on the other side of the planet
matter no less than people closer to us geographically. In the same way,
utilitarianism regards the well-being of future generations as no less
important simply because they are far away in time than the well-being
of those alive today.

A striking fact about the history of civilization is just how early in
it we appear to be. There are 5,000 years of recorded history behind us,
but how many years are potentially still to come? If we merely last as
long as the typical mammalian species, we still have 200,000 years to
go. But humans are not typical mammals, and if we can preserve our
species, there are a further one billion years until the Earth is no
longer habitable,[^15] and other planets and solar systems will be
around for trillions of years. Even on the most conservative of these
timelines, we have progressed through a tiny fraction of recorded
history. If humanity’s saga were a novel, we would still be on the first
page.

There could be astronomically more people in the future than in the
present generation. This strongly suggests that, to help people in
general, your key concern should not be to merely help the present
generation, but to ensure that the long-term future goes as well as
possible.[^16] This idea is known as _strong longtermism_:

> **Strong longtermism is the view that the most important determinant
> of the value of our actions today is how those actions affect the very
> long-run future.**

Strong longtermism is implied by most plausible forms of
utilitarianism[^17] if we assume that some of our actions can
meaningfully affect the long-term future and that we can estimate which
effects are positive and which negative. For example, there are risks to
the continued survival of the human race, including from nuclear war,
extreme climate change, man-made pathogens, and artificial general
intelligence.[^18] If we believe that the continued survival of the
human race is positive in value, then reducing the risk of human
extinction is a way of positively influencing the very long-run
future.[^19] A discussion of longtermism would go beyond the scope of
this chapter, but to learn more, we recommend reading this [academic
paper](https://globalprioritiesinstitute.org/wp-content/uploads/2019/Greaves_MacAskill_The_Case_for_Strong_Longtermism.pdf).[^20]

We discuss the implications of longtermism for ethical action in the
article [Acting on
Utilitarianism](/acting-on-utilitarianism#existential-risk-reduction).

## Respecting Commonsense Moral Norms

Sometimes people mistake utilitarianism as claiming that _one ought
always to explicitly calculate the expected value of each possible
action, and do whatever act scores highest._ Utilitarianism does _not_
in fact recommend adopting this “naïve utilitarian” decision
procedure.[^21]

Instead, as a [multi-level
theory](/types-of-utilitarianism#multi-level-utilitarianism-versus-single-level-utilitarianism),
utilitarianism specifies moral goals—criteria for objectively judging
the moral merits of an action, given all the relevant factual
details[^22]—but leaves open the question of what kind of _decision
procedure_ we should try to follow in practice. After all, it's an open
empirical question how best to actually achieve the specified moral
goals.

Utilitarianism implies that we should use whatever decision procedure
would best help us to promote overall well-being (in expectation). While
we cannot be certain what decision procedure satisfies this criterion,
we can offer some educated guesses. As psychologists Stefan Schubert and
Lucius Caviola argue in [Virtues for Real-World
Utilitarians](/guest-essays/virtues-for-real-world-utilitarians), we may
best promote overall well-being by ambitiously pursuing robustly good
[actions that effectively help others](/acting-on-utilitarianism), while
minimizing downside risks by means of commonsense virtues and
constraints.

It is widely recognized that humans are not reliable at calculating
utilities,[^23] especially when they conflict with generally-reliable
rules and heuristics (such as those prohibiting harm to others). As a
result, we cannot take rule-violating expected value calculations at
face value. Even if you've calculated that it would somehow serve the
greater good to murder your rival, you should be very skeptical that
this is true. After all, if you don't really believe that it would
overall be best for others (similarly situated) to do likewise, then you
must believe that most calculations favoring murder have actually gone
awry. So, if you've no special (symmetry-breaking) evidence establishing
that you, in particular, are the rare exception to this rule, then you
must conclude that your own murderous calculations have most likely gone
awry.[^24] Thus, absent special evidence, you should conclude that your
rule-breaking actually has lower expected value, despite your initial
calculation to the contrary.

We can be most confident that our actions have positive expected value
when we instead seek to help others in ways that are supported by good
evidence and minimize downside risk.[^25] Over the long run, we should
expect honest, cooperative altruism to do more good than ruthless
scheming for the “greater good”, because (i) historically, ruthless
schemers often do more harm than good, (ii) people rightly don't trust
ruthless schemers, and (iii) in a complex world, it's difficult to get
much done without others' trust and cooperation. If that's right, then
honest cooperative altruism systematically has _higher expected value_
than ruthless scheming, and should be preferred by utilitarians.

In summary, utilitarianism does _not_ tell us to constantly calculate
utilities and blindly follow whatever our calculations recommend. That
would be predictably counter-productive, which is contrary to the
pragmatic spirit of the theory. Instead, utilitarianism recommends
decision-procedures based on their expected value. When we are
uncertain, we should be guided by whatever decision procedure can most
reasonably (in light of everything we know about human biases and
cognitive limitations) be expected to yield better outcomes. This means
following _heuristics_ or generally-reliable rules of thumb.

As a very rough first pass, a plausible utilitarian decision-procedure
might direct us to:[^26]

1. Pursue any “low-hanging fruit” for [effectively helping
   others](/acting-on-utilitarianism) while avoiding harm,
2. Inculcate [virtues for real-world
   utilitarians](/guest-essays/virtues-for-real-world-utilitarians)
   (including respect for commonsense moral norms), and
3. In a calm moment, reflect on how we could better prioritize and
   allocate our moral efforts, including by seeking out expert
   cost-benefit analyses and other evidence to better inform our overall
   judgments of expected value.[^27]

## Conclusion

Utilitarianism has important implications for how we should think about
leading an ethical life.

The theory rejects an intrinsic moral difference between doing and
allowing harm. This position contributes to the demandingness of
utilitarianism, since it implies that whenever we decide not to help
another person, we are complicit in their misery.

By the lights of utilitarianism, we should choose carefully which moral
problems to work on and by what means, based on where we can do the most
good. We should extend our moral concern to all sentient beings, meaning
every individual capable of experiencing happiness or suffering.
Utilitarianism urges us to consider the well-being of all individuals
regardless of what species they belong to, what country they live in,
and at what point in time they exist.

Though utilitarians should try to use their lives to do the most good
they can, in practice, they should do so while respecting commonsense
moral virtues like honesty, integrity, fairness, and law-abidingness.
There are reasons we do not see utilitarians robbing banks to donate the
proceeds: these commonsense moral prohibitions help society to function
smoothly, and any naive calculation that violating such a prohibition
would promote the greater good is almost always mistaken.

The next chapter discusses important rival theories that may overlap
significantly with utilitarianism in practice.

{{< next-page-textbook title="Near-Utilitarian Alternatives"
url="/near-utilitarian-alternatives" >}}

{{< how-to-cite authors="MacAskill, W., Meissner, D., and Chappell,
R.Y." >}}

{{< button >}}

## Resources and Further Reading

### Is There a Difference Between Doing and Allowing Harm?

- [@Woollard2002DoingVs]
- [@Bennett1995ActItself]

### The Expanding Moral Circle

- [@Singer1997DrowningChildAnd]
- [@Singer1981ExpandingCircleEthics]

### Cosmopolitanism: Expanding the Moral Circle Across Geography

- [@Crashcourse2017PovertyOurResponse]
- [@Singer2009LifeYouCan]. (the book is available for free download)
- [@Singer2016FamineAffluenceMorality]
- [@Scheffler1999ConceptionsCosmopolitanism]

### Anti-Speciesism: Expanding the Moral Circle Across Species

- [@Crashcourse2017NonHumanAnimals]
- [@Singer2023AnimalLiberationNow]
- [@McMahan2003Animals]
- [@Sebo2019UtilitarianCaseFor]

### Longtermism: Expanding the Moral Circle Across Time

- [@Ord2020PrecipiceExistentialRisk]
- [@Greaves2021CaseStrongLongtermism]
- [@Beckstead2013OverwhelmingImportanceShaping]
- [@Bostrom2003AstronomicalWasteOpportunity]
- [@MacAskill2022WhatWeOwe]

### Respecting Commonsense Moral Norms

- [@Gibbard1984UtilitarianismAndHuman]
- [@Hare1981MoralThinkingIts]
- [@Mackie1984RightsUtilityAnd]
- [@Pettit1986RestrictiveConsequentialism]

[^1]: [@Sidgwick1907MethodsEthics, p. 414].
[^2]: For a discussion of demandingness in the context of global poverty
    alleviation, see [@Singer2009LifeYouCan].
[^3]: [@GiveWell2010YourDollarGoes].
[^4]: [@Berman2018ImpedimentsEffectiveAltruism].
[^5]: The case for cause neutrality is made in
    [@SentiencePolitics2016BenefitsCauseneutrality].
[^6]: [@Ord2013MoralImperativeCosteffectiveness].
[^7]: Cf. [@Singer1981ExpandingCircleEthics].
[^8]: [@Singer2011PracticalEthics, p. 50].
[^9]: Cf. [@MacAskill2015DoingGoodBetter, chap. 1].
[^10]: [@Singer2002AnimalLiberation, p. 9]. Indeed, there is
    psychological evidence suggesting that speciesism goes hand in hand
    with other discriminatory attitudes like racism, sexism, and
    homophobia: Cf. [@Caviola2019MoralStandingAnimals].

[^11]: For instance, see [@Low2012CambridgeDeclarationConsciousness].
[^12]: [@Tomasik2019HowManyWild].
[^13]:
    There is an ongoing academic debate about the moral importance of
    wild animal welfare. For example, see the following:
    [@Ng1995WelfareBiologyEvolutionary].
    [@McMahan2015MoralProblemPredation]. [@Moen2016EthicsOfWild].

[^14]:
    For a further discussion of this topic we recommend this interview
    with researcher Persis Eskander: [@Wiblin2019AnimalsWildOften].
[^15]: [@Adams2008LongtermAstrophysicalProcesses].
[^16]: For a discussion of this idea and its underlying assumptions, see
    [@Beckstead2013OverwhelmingImportanceShaping].
[^17]: Cf. [@Greaves2021CaseStrongLongtermism, sec. 4.1].
[^18]: For a detailed discussion of existential risks and the moral
    importance of the long-run future of humanity, see
    [@Ord2020PrecipiceExistentialRisk].
[^19]: For a classic paper on the importance of reducing existential
    risk, see [@Bostrom2013ExistentialRiskPrevention].
[^20]: Note that Professor William MacAskill, coauthor of this website,
    is also a coauthor of this paper.
[^21]: As [chapter 2
    explains](/types-of-utilitarianism#multi-level-utilitarianism-versus-single-level-utilitarianism):
    "[T]o our knowledge no one has ever defended single-level
    utilitarianism [i.e., using the utilitarian criterion as a universal
    decision procedure], including the classical utilitarians.
    Deliberately calculating the expected consequences of our actions is
    error-prone and risks falling into decision paralysis."

[^22]:
     Such details might simply be stipulated in a hypothetical example.
     In real life cases, our uncertainty about relevant factual details
     should generally carry over to make us similarly uncertain about
     our moral verdicts and evaluations.

[^23]:
     See, e.g., [@Mackie1984RightsUtilityAnd].

[^24]:
     In particular, you can't take at face value your inclination to
     think that there are special reasons in your case, if you believe
     that most people in subjectively similar situations are mistaken in
     taking themselves to have such special reasons. Symmetry-breaking
     evidence is evidence that _distinctively_ establishes your
     reliability in comparison to others with similar (but, in their
     case, misguided) beliefs. Note that such symmetry-breaking evidence
     is very hard to come by!

[^25]:
     That's not to say that we should strictly optimize for _confidence_
     in positive expected value: Something that's _certainly
     barely-good_ in expectation may be less worth pursuing than
     something that is _almost_ certainly high EV even if there's a
     slight risk that you've overlooked something that would mean the
     action is actually mildly negative in expectation. Such uncertainty
     could still result in higher "all things considered" expected
     value, in principle. The point is just that grounds for doubting a
     positive verdict from our initial EV calculations should rationally
     lead us to assign lower (and in some cases, even _negative_)
     expected value to that option, all things considered. And in
     practice, it seems that we should often have strong priors that
     rights-violating actions are net-negative, which a rough and
     broadly unreliable calculation should not suffice to overturn.

[^26]:
     These claims are not, strictly speaking, built into utilitarianism
     as a fundamental moral theory. Rather, we are speculating about the
     further question of _what decision-procedure has the highest
     expected value in typical circumstances_. (Note that, in principle,
     the answer may differ for different individuals in different
     circumstances. Nothing in utilitarianism requires uniformity, if
     that would not be for the best.)

[^27]: This might (but need not) include performing some “back of the
    envelope” calculations of expected value. Even then, to truly
    maximize expected value, these naive calculations must be tempered
    by constraints against ruthless scheming, given our prior judgment
    that the latter is most likely counterproductive. That is, if we
    calculate a slightly higher explicit “expected value” for one act
    than another, but the former involves egregious norm-breaking, we
    should probably conclude that the latter (safer) option is
    _actually_ better in expectation.
